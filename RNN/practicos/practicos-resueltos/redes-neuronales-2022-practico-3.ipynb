{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"redes-neuronales-2022-practico-3.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMDxCxmdc0s+08Dlu/kqu6+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Algebra lineal con **SciPy**\n","\n","Referencias:\n","\n","* https://docs.scipy.org/doc/scipy/reference/linalg.html\n","* https://www.datacamp.com/cheat-sheet/scipy-cheat-sheet-linear-algebra-in-python\n","\n","## **Ejercicio 1)** Importando librerías\n","\n","Importe las librerías `numpy` para operar con arrays, `scipy` para utilizar rutinas de algebra lineal y `matplotlib.pyplot` para graficar."],"metadata":{"id":"NRYEofSD0xoF"}},{"cell_type":"code","source":["import numpy as np\n","import scipy as sp\n","import scipy.linalg as linalg\n","import matplotlib.pyplot as plt"],"metadata":{"id":"I8N3D_nU1_oT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Ejercicio 2)** Inversa de una matriz\n","\n","Referencias:\n","\n","* https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.inv.html#scipy.linalg.inv\n","\n","**a)** Use `linalg.inv()` para calcular la inversa $A^{-1}$ de la matriz\n","\n","\\begin{equation}\n","A = \\bigg(\n","\\begin{array}{cc}\n","1 & 2 \\\\\n","3 & 4\n","\\end{array}\n","\\bigg)\n","\\end{equation}\n","\n","**b)** Use `np.dot()` para chequear que $A^{-1}A \\approx \\mathbb{I}$, donde $\\mathbb{I}$ es la matriz identidad.\n","\n","**c)** Chequee que $AA^{-1} \\approx \\mathbb{I}$."],"metadata":{"id":"NcaGEHAd10sb"}},{"cell_type":"code","source":[""],"metadata":{"id":"8NCYjM0hUyVi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Ejercicio 3)** Sistema de ecuaciones lineales\n","\n","Referencias:\n","\n","* https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.solve.html#scipy.linalg.solve\n","\n","**a)** Use `linalg.solve()` para resolver el sistema $Ax=b$ donde, $x$ es la incógnita,\n","\n","\\begin{equation}\n","A = \\left(\n","\\begin{array}{cc}\n","3 & 2 & 0 \\\\\n","1 & -1 & 0 \\\\\n","0 & 5 & 1\n","\\end{array}\n","\\right)\n","\\end{equation}\n","\n","y\n","\n","\\begin{equation}\n","b = \\left(\n","\\begin{array}{cc}\n","2 \\\\\n","4 \\\\\n","1 \n","\\end{array}\n","\\right)\n","\\end{equation}\n","\n","**b)** Chequee que $Ax=b$."],"metadata":{"id":"4pp5DryY2azA"}},{"cell_type":"code","source":[""],"metadata":{"id":"9ZdxWCm1U07s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Ejercicio 4)** Cuadrados mínimos\n","\n","Dada una matriz $X$ de dimensiones $n\\times m$ y un vector $y$ de dimensión $n$, la función `linalg.lstsq(X,y,...)` se usa para encontrar el vector $y$ de dimensión $m$ que minimize la norma L2 $|y-Xc|$. Notar que minimizar esta cantidad no necesariamente implica que $Xc=y$. Minimizar $|y-Xc|$ es útil para infinidad de problemas. En particular, es útil para ajustar polinomios a datos.\n","\n","Referencias:\n","\n","* https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lstsq.html#scipy.linalg.lstsq\n","\n","**a)** Use `linalg.lstsq(X,y)` para encontrar los coeficientes $c_0, $c_1 y $c_2$ del polinomio $p(x)=c_0+c_1x+c_2x^2$ que ajuste lo mejor posible los siguientes datos\n","\n","    x = np.array([1.0, 2.5, 3.5, 4.0, 5.0, 7.0, 8.5])\n","    y = np.array([0.3, 1.1, 1.5, 2.0, 3.2, 6.6, 8.6])\n","\n","de acuerdo al método de cuadrados mínimos\n","\n","\\begin{eqnarray}\n","\\min_{c_0,c_1,c_2} \\sum_i (y_i-p(x_i))^2\n","\\end{eqnarray}\n","\n","**Ayuda:** Utilizar la matriz $X$ de entradas $X_{ij}=x_i^j$, i.e. la $j$-ésima potencia de $x_i$.\n","\n","**b)** Con el fin de evaluar la bondad del ajuste, grafique en una misma figura:\n","\n","* con símbolos los datos y\n","* con una curva el polinomio ajustado."],"metadata":{"id":"YS7bf2SJ2nws"}},{"cell_type":"code","source":[""],"metadata":{"id":"jrPR7xIYU4V9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Ejercicio 5)** pseudo-inversa de Moore-Penrose de una matriz\n","\n","La pseudo-inversa de Moore-Penrose es una generalización de la noción de inversa, que resulta útil porque existe para cualquier matriz de dimensiones y entradas arbitrarias sobre los números reales o complejos.\n","\n","Si una matriz $A$ es invertible (y por ende cuadrada), existe una matriz $B$ tal $ABBA=\\mathbb{I}$. Dicha matríz $B$ es única (para la $A$ dada) y, por ende, se la denota por $A^{-1}$ y se la denomina inversa de $A$. En particular, notar que $AA^{-1}A = A$ y $A^{-1}AA^{-1}=A^{-1}$. Estó último es lo que pseudo-inversa de Moore-Penrose generaliza.\n","\n","Si $A$ no es invertible, luego no existe una matriz $B$ para la cual se cumpla la primera condición $AB=BA=\\mathbb{I}$. Sin embargo, siempre existe una matriz $B$ para la cual se cumple $ABA = A$ y $BAB=B$. Dicha matriz $B$ es única (para la $A$ dada) y, por ende, se la denota por $A^+$ y se la denomina por pseudo-inversa de Moore-Penrose de $A$.\n","\n","Referencias:\n","\n","* https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse\n","* https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.pinv.html#scipy.linalg.pinv\n","\n","**a)** Cree una matriz $A$ de dimensiones $n\\times m$ con $n=9$ y $m=6$ de números aleatorios generados a partir de la distribución normal de media 0 y varianza 1.\n","\n","**b)** Use `linalg.pinv()` para computar la psuedo-inversa de Moore-Penrose $A^+$ de $A$.\n","\n","**c)** Que dimensiones tiene $A^+$?\n","\n","**d)** Use `np.allclose()` para verifique que $AA^+A\\approx A$.\n","\n","**e)** Verifique que $A^+AA^+\\approx A^+$."],"metadata":{"id":"Siuu1AyTP_cF"}},{"cell_type":"code","source":[""],"metadata":{"id":"XQd7lWObU6lR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Ejercicio 5)** Producto de Kronecker\n","\n","Considere las matrices\n","\n","\\begin{equation}\n","A = \\left(\n","\\begin{array}{cc}\n","A_{11} & \\dots & A_{1m} \\\\\n","\\vdots & \\ddots & \\vdots \\\\\n","A_{nm} & \\dots & A_{nm}\n","\\end{array}\n","\\right)\n","\\end{equation}\n","\n","y\n","\n","\\begin{equation}\n","B = \\left(\n","\\begin{array}{cc}\n","B_{11} & \\dots & B_{1s} \\\\\n","\\vdots & \\ddots & \\vdots \\\\\n","B_{r1} & \\dots & B_{rs}\n","\\end{array}\n","\\right)\n","\\end{equation}\n","\n","El producto de Kronecker es la matriz de dimensiones $nr\\times ms$ dada por\n","\n","\\begin{equation}\n","A\\otimes B \n","= \n","\\left(\n","\\begin{array}{cc}\n","A_{11}B & \\dots & A_{1m}B \\\\\n","\\vdots & \\ddots & \\vdots \\\\\n","A_{n1}B & \\dots & A_{nm}B\n","\\end{array}\n","\\right)\n","= \n","\\left(\n","\\begin{array}{cc}\n","B_{11}A & \\dots & B_{1s}A \\\\\n","\\vdots & \\ddots & \\vdots \\\\\n","B_{r1}A & \\dots & B_{rs}A\n","\\end{array}\n","\\right)\n","\\end{equation}\n","\n","donde $A_{ij}B$ es un bloque de la matriz $A\\otimes B$ igual al producto del escalar $A_{ij}$ por la matriz $B$.\n","Idem para $B_{ij}A$.\n","\n","Referencias:\n","\n","* https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.kron.html#scipy.linalg.kron\n","\n","**a)** Use `linalg.kron()` para calcular el producto de Kronecker de las matrices\n","\n","\\begin{equation}\n","A = \\left(\n","\\begin{array}{cc}\n","1 & 2 \\\\\n","3 & 4 \\\\\n","\\end{array}\n","\\right)\n","\\end{equation}\n","\n","y\n","\n","\\begin{equation}\n","B = \\left(\n","\\begin{array}{cc}\n","1 \\\\\n","1 \\\\\n","1 \\\\\n","\\end{array}\n","\\right)\n","\\end{equation}\n","\n","Que dimensiones tiene $A\\otimes B$ en este caso?\n","\n","**b)** Repita para las matrices\n","\n","\\begin{equation}\n","A = \\left(\n","\\begin{array}{cc}\n","1 & 2 \\\\\n","3 & 4 \\\\\n","\\end{array}\n","\\right)\n","\\end{equation}\n","\n","y\n","\n","\\begin{equation}\n","B = \\left(\n","\\begin{array}{cc}\n","1 & 1 & 1 \n","\\end{array}\n","\\right)\n","\\end{equation}\n","\n","Que dimensiones tiene $A\\otimes B$ en este otro caso?"],"metadata":{"id":"lmq42jPWWvY0"}},{"cell_type":"code","source":[""],"metadata":{"id":"9UMr6Ot4U_ew"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Ejercicio 5)** Autovalores y autovectores\n","\n","Considere una matriz $A$ de $n\\times m$. Luego, existen un conjunto de escalares $\\{\\lambda_1,...,\\lambda_n\\}$ y un correspondiente conjunto de vectores distintos $\\{v_1,...,v_n\\}$ tales que\n","\n","\\begin{eqnarray}\n","Av_i = \\lambda_i v_i \\;\\;\\;\\; (1)\n","\\end{eqnarray}\n","\n","para todo $i\\in\\{1,...,n\\}$. Estos $\\lambda_i$ y $v_i$ se denominan autovalores y autovectores a derecha de $A$, respectivamente. Los autovalores pueden repetirse, pueden ser nulos y se los suele ordenar de mayor a menor en orden descendiente de valor absoluto: $|\\lambda_1|\\geq |\\lambda_2|\\geq ...\\geq |\\lambda_n|\\geq 0$. Los autovectores pueden normalizarse, ya que si $v_i$ es autovector, luego $v_i/|v_i|$ también lo es.\n","\n","Los autovectores constituyen una base del dominio de $A$. Esto es muy útil porque permite escribir cualquier vector $v$ en el dominio de $A$ como una combinación linal de sus autovectores\n","\n","\\begin{eqnarray}\n","v = \\sum_i c_i v_i\n","\\end{eqnarray}\n","\n","Luego, calcular el producto de $A$ con $c$ resulta muy simple\n","\n","\\begin{eqnarray}\n","Av = A\\left(\\sum_i c_i v_i\\right) = \\sum_i c_iAv_i = \\sum_i c_i\\lambda_iv_i\n","\\end{eqnarray}\n","\n","Análogamente, existen un conjunto de escalares $\\{\\mu_1,...,\\mu_m\\}$ y un correspondiente conjunto de vectores diferentes $\\{u_1,...,u_m\\}$ de $A$, llamados autovalores y autovectores a izquierda de $A$, respectivamente, y son tales que\n","\n","\\begin{eqnarray}\n","u_jA = \\mu_j u_j \\;\\;\\;\\; (2)\n","\\end{eqnarray}\n","\n","para todo $j\\in\\{1,...,m\\}$.\n","\n","**a)** Use `linalg.eigvals()` para calcular los autovalores a derecha de la matriz \n","\n","\\begin{equation}\n","A = \\left(\n","\\begin{array}{cc}\n","0 & -1 \\\\\n","1 &  0\n","\\end{array}\n","\\right)\n","\\end{equation}\n","\n","Referencias:\n","\n","* https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.eig.html#scipy.linalg.eig\n","\n","**b)** Use `linalg.eig()` para calcular los autovalores y los correspondientes autovectores a derecha de $A$.\n","\n","**c)** Verifique que la ecuación $(1)$ se cumple.\n","\n","**d)** Use `linalg.eig()` para calcular los autovalores y los correspondientes autovectores a izquierda de $A$.\n","\n","**e)** Verifique que la ecuación $(2)$ se cumple."],"metadata":{"id":"2OsaX5tGJ3QD"}},{"cell_type":"code","source":[""],"metadata":{"id":"palOLmRXVCVU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Ejercicio 6)** Singular Value Decomposition (SVD)\n","\n","Una matriz compleja $A$ de dimensiones $n\\times m$ se la puede pensar como una transformación lineal $A:V\\to W$ desde un espacio vectorial $V$ de dimensión $m$ a un espacio vectorial $W$ de dimensión $n$. \n","A $V$ se lo suele llamar espacio de salida y a $W$ espacio de llegada.\n","\n","Cualquiera sea $A$, siempre existen matrices $L$, $D$ y $R$, de dimensiones $n\\times n$, $n\\times m$ y $m\\times m$, respectivamente, tales que $U$ y $V$ son unitarias, $S$ es diagonal y\n","\n","\\begin{eqnarray}\n","A = LDR^{\\dagger} \\;\\;\\;\\; (1)\n","\\end{eqnarray}\n","\n","Los valores diagonales\n","\n","\\begin{eqnarray}\n","\\sigma_i := D_{ii}\n","\\end{eqnarray}\n","\n","están definidos para todo $i\\in \\{1,...,k\\}$, donde $k=\\min(n,m)$,\n","se los denomina valores singulares de $A$, son todos reales, no-negativos y se los suele ordenar de mayor a menor, i.e. $\\sigma_1\\geq \\sigma_2 \\geq ... \\geq \\sigma_k\\geq 0$. \n","El número de valores singulares no nulos es el rango de $A$.\n","\n","Los vectores columna $l_1,...,l_n$ de $L$ forman una base ortonormal del espacio vectorial de llegada $W$.\n","Los vectores columna $r_1,...,r_m$ de $R$ forman una base ortonormal del espacio vectorial de salida $V$.\n","Usando estas propiedades, la ecuación $(1)$ puede reescribirse como\n","\n","\\begin{eqnarray}\n","A = \\sum_{i=1}^k \\sigma_i \\, (l_i \\otimes r_i^{\\dagger}) \\;\\;\\;\\; (2)\n","\\end{eqnarray}\n","\n","donde $l_i \\otimes r_i^{\\dagger}$ es la matriz de dimensiones $n\\times m$ resultante del producto de Kronecker entre la matriz $l_i$ de dimensiones $n\\times 1$ y la matriz $r_i^{\\dagger}$ de dimensiones $1\\times m$.\n","\n","Referencias:\n","\n","* https://en.wikipedia.org/wiki/Singular_value_decomposition\n","* https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.svd.html#scipy.linalg.svd\n","\n","**a)** Cree una matriz compleja $A$ de dimensiones $n\\times m$ con $n=9$ y $m=6$, cuyas entradas $a_{ij} = x_{ij} + i y_{ij}$ se computen generando números aleatorios $x_{ij}$ e $y_{ij}$ a partir de una distribución normal de media 0 y varianza 1.\n","\n","**b)** Use `linalg.svd()` para calcular la SVD de $A$. Dicha función retorna 3 arrays, digamos `L`, `s` y `Rc`, donde `L` denota a la matriz $L$, `s` denota al vector de valores singulares $(\\sigma_1,\\sigma_2,...,\\sigma_k)$ y `Rc` denota a la matriz compleja conjugada $R^{\\dagger}$.\n","Inspeccione las dimensiones de los arrays retornados.\n","\n","**c)** Use el vector `s` para crear la matriz diagonal $D$ de dimensiones $n\\times m$.\n","\n","**d)** Use `np.allclose()` para verificar numéricamente la ecuación $(1)$."],"metadata":{"id":"hJq6cMug_8WP"}},{"cell_type":"code","source":[""],"metadata":{"id":"BAvpFeKgVEj3"},"execution_count":null,"outputs":[]}]}